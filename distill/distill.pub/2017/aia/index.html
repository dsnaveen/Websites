<!DOCTYPE html><html lang="en">
<!-- Mirrored from distill.pub/2017/aia/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 04 Feb 2018 15:37:03 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charset="utf-8"><meta name="viewport" content="width=768"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><script>
window.addEventListener('WebComponentsReady', function() {
  console.warn('WebComponentsReady');
  const loaderTag = document.createElement('script');
  loaderTag.src = '../../template.v2.js';
  document.head.insertBefore(loaderTag, document.head.firstChild);
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.0.17/webcomponents-loader.js"></script>

<style>
  .shaded-figure {
    background-color: hsl(200, 20%, 97%);
    border-top: 1px solid hsla(0, 0%, 0%, 0.1);
    border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
    padding: 30px 0;
  }
  d-article figure.tight {
    margin-top: 0;
    margin-bottom: 0;
  }
</style>

<style id="distill-prerendered-styles" type="text/css">html {
  font-size: 14px;
	line-height: 1.6em;
  /* font-family: "Libre Franklin", "Helvetica Neue", sans-serif; */
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  /*, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";*/
  text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}

@media(min-width: 768px) {
  html {
    font-size: 16px;
  }
}

body {
  margin: 0;
}

a {
  color: #004276;
}

figure {
  margin: 0;
}

table {
	border-collapse: collapse;
	border-spacing: 0;
}

table th {
	text-align: left;
}

table thead {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

table thead th {
  padding-bottom: 0.5em;
}

table tbody :first-child td {
  padding-top: 0.5em;
}

pre {
  overflow: auto;
  max-width: 100%;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}

sup, sub {
  vertical-align: baseline;
  position: relative;
  top: -0.4em;
  line-height: 1em;
}

sub {
  top: 0.4em;
}

.kicker,
.marker {
  font-size: 15px;
  font-weight: 600;
  color: rgba(0, 0, 0, 0.5);
}


/* Headline */

@media(min-width: 1024px) {
  d-title h1 span {
    display: block;
  }
}

/* Figure */

figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

figcaption+figure {

}

figure img {
  width: 100%;
}

figure svg text,
figure svg tspan {
}

figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

@media(min-width: 1024px) {
figcaption,
.figcaption {
    font-size: 13px;
  }
}

figure.external img {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

figcaption b,
figcaption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}
@supports not (display: grid) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    display: block;
    padding: 8px;
  }
}

.base-grid,
distill-header,
d-title,
d-abstract,
d-article,
d-appendix,
distill-appendix,
d-byline,
d-footnote-list,
d-citation-list,
distill-footer {
  display: grid;
  justify-items: stretch;
  grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
  grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1280px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}




.base-grid {
  grid-column: screen;
}

/* .l-body,
d-article > *  {
  grid-column: text;
}

.l-page,
d-title > *,
d-figure {
  grid-column: page;
} */

.l-gutter {
  grid-column: gutter;
}

.l-text,
.l-body {
  grid-column: text;
}

.l-page {
  grid-column: page;
}

.l-body-outset {
  grid-column: middle;
}

.l-page-outset {
  grid-column: page;
}

.l-screen {
  grid-column: screen;
}

.l-screen-inset {
  grid-column: screen;
  padding-left: 16px;
  padding-left: 16px;
}


/* Aside */

d-article aside {
  grid-column: gutter;
  font-size: 12px;
  line-height: 1.6em;
  color: rgba(0, 0, 0, 0.6)
}

@media(min-width: 768px) {
  aside {
    grid-column: gutter;
  }

  .side {
    grid-column: gutter;
  }
}
d-title {
  padding: 2rem 0 1.5rem;
  contain: layout style;
  overflow-x: hidden;
}

@media(min-width: 768px) {
  d-title {
    padding: 4rem 0 1.5rem;
  }
}

d-title h1 {
  grid-column: text;
  font-size: 40px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

@media(min-width: 768px) {
  d-title h1 {
    font-size: 50px;
  }
}

d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  grid-column: text;
}

d-title .status {
  margin-top: 0px;
  font-size: 12px;
  color: #009688;
  opacity: 0.8;
  grid-column: kicker;
}

d-title .status span {
  line-height: 1;
  display: inline-block;
  padding: 6px 0;
  border-bottom: 1px solid #80cbc4;
  font-size: 11px;
  text-transform: uppercase;
}
d-byline {
  contain: content;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}


d-byline .byline {
  grid-template-columns: 1fr 1fr;
  grid-column: text;
}

@media(min-width: 768px) {
  d-byline .byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
  }
}

d-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
  margin-bottom: 1em;
}

@media(min-width: 768px) {
  d-byline .authors-affiliations {
    margin-bottom: 0;
  }
}

d-byline h3 {
  font-size: 0.6rem;
  font-weight: 400;
  color: rgba(0, 0, 0, 0.5);
  margin: 0;
  text-transform: uppercase;
}

d-byline p {
  margin: 0;
}

d-byline a,
d-article d-byline a {
  color: rgba(0, 0, 0, 0.8);
  text-decoration: none;
  border-bottom: none;
}

d-article d-byline a:hover {
  text-decoration: underline;
  border-bottom: none;
}

d-byline p.author {
  font-weight: 500;
}

d-byline .affiliations {

}
d-article {
  contain: layout style;
  overflow-x: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  padding-top: 2rem;
  color: rgba(0, 0, 0, 0.8);
}

d-article > * {
  grid-column: text;
}

@media(min-width: 768px) {
  d-article {
    font-size: 16px;
  }
}

@media(min-width: 1024px) {
  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
}


/* H2 */


d-article .marker {
  text-decoration: none;
  border: none;
  counter-reset: section;
  grid-column: kicker;
  line-height: 1.7em;
}

d-article .marker:hover {
  border: none;
}

d-article .marker span {
  padding: 0 3px 4px;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  position: relative;
  top: 4px;
}

d-article .marker:hover span {
  color: rgba(0, 0, 0, 0.7);
  border-bottom: 1px solid rgba(0, 0, 0, 0.7);
}

d-article h2 {
  font-weight: 600;
  font-size: 24px;
  line-height: 1.25em;
  margin: 2rem 0 1.5rem 0;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding-bottom: 1rem;
}

@media(min-width: 1024px) {
  d-article h2 {
    font-size: 36px;
  }
}

/* H3 */

d-article h3 {
  font-weight: 700;
  font-size: 18px;
  line-height: 1.4em;
  margin-bottom: 1em;
  margin-top: 2em;
}

@media(min-width: 1024px) {
  d-article h3 {
    font-size: 20px;
  }
}

/* H4 */

d-article h4 {
  font-weight: 600;
  text-transform: uppercase;
  font-size: 14px;
  line-height: 1.4em;
}

d-article a {
  color: inherit;
}

d-article p,
d-article ul,
d-article ol,
d-article blockquote {
  margin-top: 0;
  margin-bottom: 1em;
  margin-left: 0;
  margin-right: 0;
}

d-article blockquote {
  border-left: 2px solid rgba(0, 0, 0, 0.2);
  padding-left: 2em;
  font-style: italic;
  color: rgba(0, 0, 0, 0.6);
}

d-article a {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  text-decoration: none;
}

d-article a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.8);
}

d-article .link {
  text-decoration: underline;
  cursor: pointer;
}

d-article ul,
d-article ol {
  padding-left: 24px;
}

d-article li {
  margin-bottom: 1em;
  margin-left: 0;
  padding-left: 0;
}

d-article li:last-child {
  margin-bottom: 0;
}

d-article pre {
  font-size: 14px;
  margin-bottom: 20px;
}

d-article hr {
  grid-column: screen;
  width: 100%;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article section {
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article span.equation-mimic {
  font-family: georgia;
  font-size: 115%;
  font-style: italic;
}

d-article > d-code,
d-article section > d-code  {
  display: block;
}

d-article > d-math[block],
d-article section > d-math[block]  {
  display: block;
}

@media (max-width: 768px) {
  d-article > d-code,
  d-article section > d-code,
  d-article > d-math[block],
  d-article section > d-math[block] {
      overflow-x: scroll;
      -ms-overflow-style: none;  // IE 10+
      overflow: -moz-scrollbars-none;  // Firefox
  }

  d-article > d-code::-webkit-scrollbar,
  d-article section > d-code::-webkit-scrollbar,
  d-article > d-math[block]::-webkit-scrollbar,
  d-article section > d-math[block]::-webkit-scrollbar {
    display: none;  // Safari and Chrome
  }
}

d-article .citation {
  color: #668;
  cursor: pointer;
}

d-include {
  width: auto;
  display: block;
}

d-figure {
  contain: layout style;
}

/* KaTeX */

.katex, .katex-prerendered {
  contain: content;
  display: inline-block;
}

/* Tables */

d-article table {
  border-collapse: collapse;
  margin-bottom: 1.5rem;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table th {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table td {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

d-article table tr:last-of-type td {
  border-bottom: none;
}

d-article table th,
d-article table td {
  font-size: 15px;
  padding: 2px 8px;
}

d-article table tbody :first-child td {
  padding-top: 2px;
}
span.katex-display {
  text-align: left;
  padding: 8px 0 8px 0;
  margin: 0.5em 0 0.5em 1em;
}

span.katex {
  -webkit-font-smoothing: antialiased;
  color: rgba(0, 0, 0, 0.8);
  font-size: 1.18em;
}
@media print {

  @page {
    size: 8in 11in;
    @bottom-right {
      content: counter(page) " of " counter(pages);
    }
  }

  html {
    /* no general margins -- CSS Grid takes care of those */
  }

  p, code {
    page-break-inside: avoid;
  }

  h2, h3 {
    page-break-after: avoid;
  }

  d-header {
    visibility: hidden;
  }

  d-footer {
    display: none!important;
  }

}
</style><link rel="stylesheet" href="../../third-party/katex/katex.min.css" crossorigin="anonymous">
    
    <link rel="icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA99JREFUeNrsG4t1ozDMzQSM4A2ODUonKBucN2hugtIJ6E1AboLcBiQTkJsANiAb9OCd/OpzMWBJBl5TvaeXPiiyJetry0J8wW3D3QpjRh3GjneXDq+fSQA9s2mH9x3KDhN4foJfCb8N/Jrv+2fnDn8vLRQOplWHVYdvHZYdZsBcZP1vBmh/n8DzEmhUQDPaOuP9pFuY+JwJHwHnCLQE2tnWBGEyXozY9xCUgHMhhjE2I4heVWtgIkZ83wL6Qgxj1obfWBxymPwe+b00BCCRNPbwfb60yleAkkBHGT5AEehIYz7eJrFDMF9CvH4wwhcGHiHMneFvLDQwlwvMLQq58trRcYBWfYn0A0OgHWQUSu25mE+BnoYKnnEJoeIWAifzOv7vLWd2ZKRfWAIme3tOiUaQ3UnLkb0xj1FxRIeEGKaGIHOs9nEgLaaA9i0JRYo1Ic67wJW86KSKE/ZAM8KuVMk8ITVhmxUxJ3Cl2xlm9Vtkeju1+mpCQNxaEGNCY8bs9X2YqwNoQeGjBWut/ma0QAWy/TqAsHx9wSya3I5IRxOfTC+leG+kA/4vSeEcGBtNUN6byhu3+keEZCQJUNh8MAO7HL6H8pQLnsW/Hd4T4lv93TPjfM7A46iEEqbB5EDOvwYNW6tGNZzT/o+CZ6sqZ6wUtR/wf7mi/VL8iNciT6rHih48Y55b4nKCHJCCzb4y0nwFmin3ZEMIoLfZF8F7nncFmvnWBaBj7CGAYA/WGJsUwHdYqVDwAmNsUgAx4CGgAA7GOOxADYOFWOaIKifuVYzmOpREqA21Mo7aPsgiY1PhOMAmxtR+AUbYH3Id2wc0SAFIQTsn9IUGWR8k9jx3vtXSiAacFxTAGakBk9UudkNECd6jLe+6HrshshvIuC6IlLMRy7er+JpcKma24SlE4cFZSZJDGVVrsNvitQhQrDhW0jfiOLfFd47C42eHT56D/BK0To+58Ahj+cAT8HT1UWlfLZCCd/uKawzU0Rh2EyIX/Icqth3niG8ybNroezwe6khdCNxRN+l4XGdOLVLlOOt2hTRJlr1ETIuMAltVTMz70mJrkdGAaZLSmnBEqmAE32JCMmuTlCnRgsBENtOUpHhvvsYIL0ibnBkaC6QvKcR7738GKp0AKnim7xgUSNv1bpS8QwhBt8r+EP47v/oyRK/S34yJ9nT+AN0Tkm4OdB9E4BsmXM3SnMlRFUrtp6IDpV2eKzdYvF3etm3KhQksbOLChGkSmcBdmcEwvqkrMy5BzL00NZeu3qPYJOOuCc+5NjcWKXQxFvTa3NoXJ4d8in7fiAUuTt781dkvuHX4K8AA2Usy7yNKLy0AAAAASUVORK5CYII=
">
    <link href="../../rss.xml" rel="alternate" type="application/rss+xml" title="Articles from Distill">
  
    <title>Using Artificial Intelligence to Augment Human Intelligence</title>
    
    <link rel="canonical" href="index.html">
    
    <!--  https://schema.org/Article -->
    <meta property="description" itemprop="description" content="By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning.">
    <meta property="article:published" itemprop="datePublished" content="2017-12-04">
    <meta property="article:created" itemprop="dateCreated" content="2017-12-04">
    
    <meta property="article:modified" itemprop="dateModified" content="2017-12-21T21:52:05.000Z">
    
    <meta property="article:author" content="Shan Carter">
    <meta property="article:author" content="Michael Nielsen">
    <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Using Artificial Intelligence to Augment Human Intelligence">
    <meta property="og:description" content="By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning.">
    <meta property="og:url" content="https://distill.pub/2017/aia">
    <meta property="og:image" content="https://distill.pub/2017/aia/thumbnail.jpg">
    <meta property="og:locale" content="en_US">
    <meta property="og:site_name" content="Distill">
  
    <!--  https://dev.twitter.com/cards/types/summary -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Using Artificial Intelligence to Augment Human Intelligence">
    <meta name="twitter:description" content="By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning.">
    <meta name="twitter:url" content="https://distill.pub/2017/aia">
    <meta name="twitter:image" content="https://distill.pub/2017/aia/thumbnail.jpg">
    <meta name="twitter:image:width" content="560">
    <meta name="twitter:image:height" content="295">
  
      <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
    <meta name="citation_title" content="Using Artificial Intelligence to Augment Human Intelligence">
    <meta name="citation_fulltext_html_url" content="https://distill.pub/2017/aia">
    <meta name="citation_volume" content="2">
    <meta name="citation_issue" content="12">
    <meta name="citation_firstpage" content="e9">
    <meta name="citation_doi" content="10.23915/distill.00009">
    <meta name="citation_journal_title" content="Distill">
    <meta name="citation_journal_abbrev" content="Distill">
    <meta name="citation_issn" content="2476-0757">
    <meta name="citation_fulltext_world_readable" content="">
    <meta name="citation_online_date" content="2017/12/04">
    <meta name="citation_publication_date" content="2017/12/04">
    <meta name="citation_author" content="Carter, Shan">
    <meta name="citation_author_institution" content="Google Brain Team">
    <meta name="citation_author" content="Nielsen, Michael">
    <meta name="citation_author_institution" content="YC Research">
    <meta name="citation_reference" content="citation_title=Augmenting Human Intellect: A Conceptual Framework;citation_author=Douglas C. Engelbart;citation_publication_date=1962;">
    <meta name="citation_reference" content="citation_title=deeplearn.js font demo;citation_author=James Wexler;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Auto-encoding variational Bayes;citation_author=Diederik P. Kingma;citation_author=Max Welling;citation_publication_date=2014;citation_journal_title=ICLR;">
    <meta name="citation_reference" content="citation_title=Analyzing 50k fonts using deep neural networks;citation_author=Erik Bernhardsson;citation_publication_date=2016;">
    <meta name="citation_reference" content="citation_title=Autoencoding beyond pixels using a learned similarity metric;citation_author=Anders Boesen Lindbo Larsen;citation_author=Søren Kaae Sønderby;citation_author=Hugo Larochelle;citation_author=Ole Winther;citation_publication_date=2016;citation_journal_title=ICML;">
    <meta name="citation_reference" content="citation_title=Sampling Generative Networks;citation_author=Tom White;citation_publication_date=2016;citation_arxiv_id=1609.04468;">
    <meta name="citation_reference" content="citation_title=Writing with the Machine;citation_author=Robin Sloan;citation_publication_date=2017;citation_journal_title=Eyeo;">
    <meta name="citation_reference" content="citation_title=Automatic chemical design using a data-driven continuous representation of molecules;citation_author=Rafael Gómez-Bombarelli;citation_author=David Duvenaud;citation_author=José Miguel Hernández-Lobato;citation_author=Jorge Aguilera-Iparraguirre;citation_author=Timothy D. Hirzel;citation_author=Ryan P. Adams;citation_author=Alán Aspuru-Guzik;citation_publication_date=2016;citation_arxiv_id=1610.02415;">
    <meta name="citation_reference" content="citation_title=Generative visual manipulation on the natural image manifold;citation_author=Jun-Yan Zhu;citation_author=Philipp Krähenbühl;citation_author=Eli Schechtman;citation_author=Alexei A. Efros;citation_publication_date=2016;citation_journal_title=European Conference on Computer Vision (ECCV);">
    <meta name="citation_reference" content="citation_title=Generative adversarial nets;citation_author=Ian J. Goodfellow;citation_author=Jean Pouget-Abadie;citation_author=Mehdi Mirza;citation_author=Bing Xu;citation_author=David Warde-Farley;citation_author=Sherjil Ozair;citation_author=Aaron Courville;citation_author=Yoshua Bengio;citation_publication_date=2014;citation_journal_title=Advances in Neural Information Processing Systems (NIPS);">
    <meta name="citation_reference" content="citation_title=A Neural Representation of Sketch Drawings;citation_author=David Ha;citation_author=Douglas Eck;citation_publication_date=2017;citation_arxiv_id=1704.03477;">
    <meta name="citation_reference" content="citation_title=Real-time human interaction with supervised learning algorithms for music composition and performance;citation_author=Rebecca Fiebrink;citation_publication_date=2011;">
    <meta name="citation_reference" content="citation_title=TopoSketch: Drawing in Latent Space;citation_author=Ian Loh;citation_author=Tom White;citation_publication_date=2017;citation_journal_title=NIPS Workshop on Machine Learning for Creativity and Design;">
    <meta name="citation_reference" content="citation_title=Taking The Robots To Design School, Part 1;citation_author=Jon Gold;citation_publication_date=2016;">
    <meta name="citation_reference" content="citation_title=Hierarchical Variational Autoencoders for Music;citation_author=Adam Roberts;citation_author=Jesse Engel;citation_author=Douglas Eck;citation_publication_date=2017;citation_journal_title=NIPS Workshop on Machine Learning for Creativity and Design;">
    <meta name="citation_reference" content="citation_title=Computational creativity: the final frontier?;citation_author=Simon Colton;citation_author=Geraint A. Wiggins;citation_publication_date=2012;citation_journal_title=ECAI;">
    <meta name="citation_reference" content="citation_title=Interactive machine learning: letting users build classifiers;citation_author=Malcolm Ware;citation_author=Eibe Frank;citation_author=Geoffrey Holmes;citation_author=Mark Hall;citation_author=Ian H. Witten;citation_publication_date=2001;citation_journal_title=International Journal of Human-Computer Studies;citation_volume=55;">
    <meta name="citation_reference" content="citation_title=Eccentric School of Painting Increased Its Vogue in the Current Art Exhibition &amp;mdash; What Its Followers Attempt to Do;citation_publication_date=1911;citation_journal_title=The New York Times;">
    <meta name="citation_reference" content="citation_title=Genius: The Life and Science of Richard Feynman;citation_author=James Gleick;citation_publication_date=1992;">
    <meta name="citation_reference" content="citation_title=Thought as a Technology;citation_author=Michael Nielsen;citation_publication_date=2016;">
    <meta name="citation_reference" content="citation_title=InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets;citation_author=Xi Chen;citation_author=Yan Duan;citation_author=Rein Houthooft;citation_author=John Schulman;citation_author=Ilya Sutskever;citation_author=Pieter Abbeel;citation_publication_date=2016;citation_journal_title=NIPS;">
    <meta name="citation_reference" content="citation_title=Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks;citation_author=Alec Radford;citation_author=Luke Metz;citation_author=Soumith Chintala;citation_publication_date=2016;citation_arxiv_id=1511.06434;">
    <meta name="citation_reference" content="citation_title=Image-to-Image Translation with Conditional Adversarial Networks;citation_author=Phillip Isola;citation_author=Jun-Yan Zhu;citation_author=Tinghui Zhou;citation_author=Alexei A. Efros;citation_publication_date=2017;citation_arxiv_id=1611.07004;">
</head><body distill-prerendered=""><distill-header></distill-header>

<d-front-matter>
  <script type="text/json">{
    "title": "Using Artificial Intelligence to Augment Human Intelligence",
    "description": "By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning.",
    "authors": [
    {
    "author": "Shan Carter",
    "authorURL": "http://shancarter.com/",
    "affiliation": "Google Brain Team",
    "affiliationURL": "https://g.co/brain"
    },
    {
    "author": "Michael Nielsen",
    "authorURL": "http://michaelnielsen.org",
    "affiliation": "YC Research",
    "affiliationURL": "https://ycr.org/"
    }
    ],
    "katex": {
      "delimiters" : [
        {"left": "$", "right": "$", "display": false}
      ]
    }
  }</script>
</d-front-matter>

<d-title>
  <h1 style="grid-column: text/page;">Using Artiﬁcial Intelligence to Augment Human Intelligence</h1>
  <p>
    By creating user interfaces which let us work with the
    representations inside machine learning models, we can give
    people new tools for reasoning.
  </p>
</d-title>
<d-byline>
  <div class="byline grid">
    <div class="authors-affiliations grid">
      <h3>Authors</h3>
      <h3>Afﬁliations</h3>
      
        <p class="author">
          
            <a class="name" href="http://shancarter.com/">Shan Carter</a>
        </p>
        <p class="affiliation">
          
            <a class="affiliation" href="https://g.co/brain">Google Brain Team</a>
        </p>
      
        <p class="author">
          
            <a class="name" href="http://michaelnielsen.org/">Michael Nielsen</a>
        </p>
        <p class="affiliation">
          
            <a class="affiliation" href="https://ycr.org/">YC Research</a>
        </p>
      
    </div>
    <div>
      <h3>Published</h3>
      
        <p>Dec. 4, 2017</p> 
    </div>
    <div>
      <h3>DOI</h3>
      
        <p><a href="https://doi.org/10.23915/distill.00009">10.23915/distill.00009</a></p>
    </div>
  </div>
</d-byline>
<d-article>
  <h2>
    What are computers for?
  </h2>

  <p>
    Historically, different answers to this question – that is,
    different visions of computing – have helped inspire and
    determine the computing systems humanity has ultimately
    built. Consider the early electronic computers.  ENIAC, the
    world’s ﬁrst general-purpose electronic computer, was
    commissioned to compute artillery ﬁring tables for the United
    States Army.  Other early computers were also used to solve
    numerical problems, such as simulating nuclear explosions,
    predicting the weather, and planning the motion of rockets. The
    machines operated in a batch mode, using crude input and output
    devices, and without any real-time interaction.  It was a vision
    of computers as number-crunching machines, used to speed up
    calculations that would formerly have taken weeks, months, or more
    for a team of humans.
  </p>

  <p>
    In the 1950s a different vision of what computers are for began to
    develop.  That vision was crystallized in 1962, when Douglas
    Engelbart proposed that computers could be used as a way
    of <d-cite key="Engelbart1962a">augmenting human
    intellect</d-cite>.  In this view, computers weren’t primarily
    tools for solving number-crunching problems.  Rather, they were
    real-time interactive systems, with rich inputs and outputs, that
    humans could work with to support and expand their own
    problem-solving process.  This vision of intelligence augmentation
    (IA) deeply inﬂuenced many others, including researchers such as
    Alan Kay at Xerox PARC, entrepreneurs such as Steve Jobs at Apple,
    and led to many of the key ideas of modern computing systems. Its
    ideas have also deeply inﬂuenced digital art and music, and
    ﬁelds such as interaction design, data visualization,
    computational creativity, and human-computer interaction.
  </p>

  <p>
    Research on IA has often been in competition with research on
    artiﬁcial intelligence (AI): competition for funding, competition
    for the interest of talented researchers.  Although there has
    always been overlap between the ﬁelds, IA has typically focused
    on building systems which put humans and machines to work
    together, while AI has focused on complete outsourcing of
    intellectual tasks to machines.  In particular, problems in AI are
    often framed in terms of matching or surpassing human performance:
    beating humans at chess or Go; learning to recognize speech and
    images or translating language as well as humans; and so on.
  </p>

  <p>
    This essay describes a new ﬁeld, emerging today out of a
    synthesis of AI and IA.  For this ﬁeld, we suggest the
    name <em>artiﬁcial intelligence augmentation</em> (AIA): the use
    of AI systems to help develop new methods for intelligence
    augmentation.  This new ﬁeld introduces important new fundamental
    questions, questions not associated to either parent ﬁeld.  We
    believe the principles and systems of AIA will be radically
    different to most existing systems.
  </p>

  <p>
    Our essay begins with a survey of recent technical work hinting at
    artiﬁcial intelligence augmentation, including work
    on <em>generative interfaces</em> – that is, interfaces
    which can be used to explore and visualize generative machine
    learning models.  Such interfaces develop a kind of cartography of
    generative models, ways for humans to explore and make meaning
    from those models, and to incorporate what those models
    “know” into their creative work.
  </p>

  <p>
    Our essay is not just a survey of technical work.  We believe now
    is a good time to identify some of the broad, fundamental
    questions at the foundation of this emerging ﬁeld.  To what
    extent are these new tools enabling creativity?  Can they be used
    to generate ideas which are truly surprising and new, or are the
    ideas cliches, based on trivial recombinations of existing ideas?
    Can such systems be used to develop fundamental new interface
    primitives?  How will those new primitives change and expand the
    way humans think?
  </p>

  <h2>
    Using generative models to invent meaningful creative operations
  </h2>

  <p>
    Let’s look at an example where a machine learning model makes a
    new type of interface possible. To understand the interface,
    imagine you’re a type designer, working on creating a new
    font<d-footnote>We shall egregiously abuse the distinction between
    a font and a typeface. Apologies to any type designers who may be
    reading.</d-footnote>.  After sketching some initial designs, you
    wish to experiment with bold, italic, and condensed variations.
    Let’s examine a tool to generate and explore such variations, from
    any initial design.  For reasons that will soon be explained the
    quality of results is quite crude; please bear with us.
  </p>

  <figure class="l-screen shaded-figure">
    <d-figure id="boldify"></d-figure>
  </figure>

  <p>
    Of course, varying the bolding (i.e., the weight), italicization
    and width are just three ways you can vary a font.  Imagine that
    instead of building specialized tools, users could build their own
    tool merely by choosing examples of existing fonts.  For instance,
    suppose you wanted to vary the degree of seriﬁng on a font.  In
    the following, please select 5 to 10 sans-serif fonts from the top
    box, and drag them to the box on the left.  Select 5 to 10 serif
    fonts and drag them to the box on the right. As you do this, a
    machine learning model running in your browser will automatically
    infer from these examples how to interpolate your starting font in
    either the serif or sans-serif direction:
  </p>


  <figure id="serifify" class="l-screen shaded-figure">
  </figure>

  <p>
    In fact, we used this same technique to build the earlier bolding
    italicization, and condensing tool. To do so, we used the
    following examples of bold and non-bold fonts, of italic and
    non-italic fonts, and of condensed and non-condensed fonts:
  </p>

  <figure id="boldify-with-examples" class="l-screen">
  </figure>

  <p>
    To build these tools, we used what’s called a <em>generative
      model</em>; the particular model we use was trained
      by <d-cite key="Wexler2017">James Wexler</d-cite>.  To
      understand generative models, consider that <em>a priori</em>
      describing a font appears to require a lot of data. For
      instance, if the font is <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>6</mn><mn>4</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathrm">6</span><span class="mord mathrm">4</span></span></span></span></span> by <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>6</mn><mn>4</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathrm">6</span><span class="mord mathrm">4</span></span></span></span></span> pixels, then we’d expect
      to need <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>6</mn><mn>4</mn><mo>×</mo><mn>6</mn><mn>4</mn><mo>=</mo><mn>4</mn><mo separator="true">,</mo><mn>0</mn><mn>9</mn><mn>6</mn></mrow><annotation encoding="application/x-tex">64 \times 64 = 4,096</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathrm">6</span><span class="mord mathrm">4</span><span class="mbin">×</span><span class="mord mathrm">6</span><span class="mord mathrm">4</span><span class="mrel">=</span><span class="mord mathrm">4</span><span class="mpunct">,</span><span class="mord mathrm">0</span><span class="mord mathrm">9</span><span class="mord mathrm">6</span></span></span></span></span> parameters to describe a single
      glyph. But we can use a generative model to ﬁnd a much simpler
      description.
  </p>

  <p>
    We do this by building a neural network which takes a small number
    of input variables, called <em>latent variables</em>, and produces
    as output the entire glyph.  For the particular model we use, we
    have <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">40</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathrm">4</span><span class="mord mathrm">0</span></span></span></span></span> latent space dimensions, and map that into the
    <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo separator="true">,</mo><mn>0</mn><mn>9</mn><mn>6</mn></mrow><annotation encoding="application/x-tex">4,096</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathrm">4</span><span class="mpunct">,</span><span class="mord mathrm">0</span><span class="mord mathrm">9</span><span class="mord mathrm">6</span></span></span></span></span>-dimensional space describing all the pixels in the glyph.
    In other words, the idea is to map a low-dimensional space into a
    higher-dimensional space:
  </p>

  <figure id="latent-space-typeface" class="l-screen tight">
  </figure>

  <p>
    The generative model we use is a type of neural network known as
    a <d-cite key="Kingma2014a">variational autoencoder
    (VAE)</d-cite>.  For our purposes, the details of the generative
    model aren’t so important.  The important thing is that by
    changing the latent variables used as input, it’s possible to get
    different fonts as output.  So one choice of latent variables will
    give one font, while another choice will give a different font:
  </p>

  <figure id="latent-space-typeface-sample" class="l-screen tight">
  </figure>

  <p>
    You can think of the latent variables as a compact, high-level
    representation of the font.  The neural network takes that
    high-level representation and converts it into the full pixel
    data. It’s remarkable that just <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">40</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathrm">4</span><span class="mord mathrm">0</span></span></span></span></span> numbers can capture the
    apparent complexity in a glyph, which originally required <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo separator="true">,</mo><mn>0</mn><mn>9</mn><mn>6</mn></mrow><annotation encoding="application/x-tex">4,096</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathrm">4</span><span class="mpunct">,</span><span class="mord mathrm">0</span><span class="mord mathrm">9</span><span class="mord mathrm">6</span></span></span></span></span>
    variables.
  </p>

  <p>
    The generative model we use is learnt from a training set of more
    than <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">50</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathrm">5</span><span class="mord mathrm">0</span></span></span></span></span> thousand
    fonts <d-cite key="Bernhardsson2016a">Bernhardsson</d-cite>
    scraped from the open web.  During training, the weights and
    biases in the network are adjusted so that the network can output
    a close approximation to any desired font from the training set,
    provided a suitable choice of latent variables is made.  In some
    sense, the model is learning a highly compressed representation of
    all the training fonts.
  </p>

  <p>
    In fact, the model doesn’t just reproduce the training fonts.  It
    can also generalize, producing fonts not seen in training.  By
    being forced to ﬁnd a compact description of the training
    examples, the neural net learns an abstract, higher-level model of
    what a font is.  That higher-level model makes it possible to
    generalize beyond the training examples already seen, to produce
    realistic-looking fonts.
  </p>

  <p>
    Ideally, a good generative model would be exposed to a relatively
    small number of training examples, and use that exposure to
    generalize to the space of all possible human-readable fonts.
    That is, for any conceivable font – whether existing or
    perhaps even imagined in the future – it would be possible
    to ﬁnd latent variables corresponding exactly to that font.  Of
    course, the model we’re using falls far short of this ideal
    – a particularly egregious failure is that many fonts
    generated by the model omit the tail on the capital
    “Q” (you can see this in the examples above).  Still,
    it’s useful to keep in mind what an ideal generative model would
    do.
  </p>

  <p>
    Such generative models are similar in some ways to how scientiﬁc
    theories work. Scientiﬁc theories often greatly simplify the
    description of what appear to be complex phenomena, reducing large
    numbers of variables to just a few variables from which many
    aspects of system behavior can be deduced.  Furthermore, good
    scientiﬁc theories sometimes enable us to generalize to discover
    new phenomena.
  </p>

  <p>
    As an example, consider ordinary material objects. Such objects
    have what physicists call a <em>phase</em> – they may be a
    liquid, a solid, a gas, or perhaps something more exotic, like a
    superconductor
    or <a href="https://en.wikipedia.org/wiki/Bose–Einstein_condensate">Bose-Einstein
    condensate</a>.  <em>A priori</em>, such systems seem immensely
    complex, involving perhaps <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mn>2</mn><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{23}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">2</span><span class="mord mathrm mtight">3</span></span></span></span></span></span></span></span></span></span></span></span></span> or so molecules.  But the
    laws of thermodynamics and statistical mechanics enable us to ﬁnd
    a simpler description, reducing that complexity to just a few
    variables (temperature, pressure, and so on), which encompass much
    of the behavior of the system.  Furthermore, sometimes it’s
    possible to generalize, predicting unexpected new phases of
    matter.  For example, in 1924, physicists used thermodynamics and
    statistical mechanics to predict a remarkable new phase of matter,
    Bose-Einstein condensation, in which a collection of atoms may all
    occupy identical quantum states, leading to surprising large-scale
    quantum interference effects. We’ll come back to this predictive
    ability in our later discussion of creativity and generative
    models.
  </p>

  <p>
    Returning to the nuts and bolts of generative models, how can we
    use such models to do example-based reasoning like that in the
    tool shown above?  Let’s consider the case of the bolding tool. In
    that instance, we take the average of all the latent vectors for
    the user-speciﬁed bold fonts, and the average for all the
    user-speciﬁed non-bold fonts.  We then compute the difference
    between these two average vectors:
  </p>

  <figure id="boldify-vector-defined" class="tight" style="grid-column:middle">
  </figure>

  <p>
    We’ll refer to this as the <em>bolding vector</em>.  To make some
    given font bolder, we simply add a little of the bolding vector to
    the corresponding latent vector, with the amount of bolding vector
    added controlling the boldness of the result<d-footnote>In
    practice, sometimes a slightly different procedure is used.  In
    some generative models the latent vectors satisfy some constraints
    – for instance, they may all be of the same length.  When
    that’s the case, as in our model, a more sophisticated
    “adding” operation must be used, to ensure the length
    remains the same. But conceptually, the picture of adding the
    bolding vector is the right way to think.</d-footnote>:
  </p>

  <figure id="boldify-vector-sampled" class="tight" style="grid-column:middle">
  </figure>

  <p>
    This technique was introduced
    by <d-cite key="Larsen2016a">Larsen <em>et al</em></d-cite>, and
    vectors like the bolding vector are sometimes called
    <em>attribute vectors</em>.  The same idea is use to implement all
    the tools we’ve shown.  That is, we use example fonts to creating
    a bolding vector, an italicizing vector, a condensing vector, and
    a user-deﬁned serif vector.  The interface thus provides a way of
    exploring the latent space in those four directions.
  </p>

  <p>
    The tools we’ve shown have many drawbacks.  Consider the following
    example, where we start with an example glyph, in the middle, and
    either increase or decrease the bolding (on the right and left,
    respectively):
  </p>

  <figure id="boldify-tool-examples">
  </figure>

  <p>
    Examining the glyphs on the left and right we see many unfortunate
    artifacts.  Particularly for the rightmost glyph, the edges start to get
    rough, and the serifs begin to disappear.  A better generative
    model would reduce those artifacts.  That’s a good long-term
    research program, posing many intriguing problems.  But even with
    the model we have, there are also some striking beneﬁts to the
    use of the generative model.
  </p>

  <p>
    To understand these beneﬁts, consider a naive approach to
    bolding, in which we simply add some extra pixels around a glyph’s
    edges, thickening it up.  While this thickening perhaps matches a
    non-expert’s way of thinking about type design, an expert does
    something much more involved.  In the following we show the
    results of this naive thickening procedure versus what is actually
    done, for Georgia and Helvetica:
  </p>

  <figure id="bold-naive">
  </figure>

  <p>
    As you can see, the naive bolding procedure produces quite
    different results, in both cases.  For example, in Georgia, the
    left stroke is only changed slightly by bolding, while the right
    stroke is greatly enlarged, but only on one side.  In both
    fonts, bolding doesn’t change the height of the font, while the
    naive approach does.
  </p>

  <p>
    As these examples show, good bolding is <em>not</em> a trivial
    process of thickening up a font. Expert type designers have many
    heuristics for bolding, heuristics inferred from much previous
    experimentation, and careful study of historical
    examples. Capturing all those heuristics in a conventional program
    would involve immense work. The beneﬁt of using the generative
    model is that it automatically learns many such heuristics.
  </p>

  <p>
    For example, a naive bolding tool would rapidly ﬁll in the
    enclosed negative space in the enclosed upper region of the letter
    “A”. The font tool doesn’t do this.  Instead, it goes
    to some trouble to preserve the enclosed negative space, moving
    the A’s bar down, and ﬁlling out the interior strokes more slowly
    than the exterior.  This principle is evident in the examples
    shown above, especially Helvetica, and it can also be seen in the
    operation of the font tool:
  </p>

  <figure id="boldify-tool-examples-positive">
  </figure>

  <p>
    The heuristic of preserving enclosed negative space is not <em>a
    priori</em> obvious.  However, it’s done in many professionally
    designed fonts.  If you examine examples like those shown above
    it’s easy to see why: it improves legibility.  During training,
    our generative model has automatically inferred this principle
    from the examples it’s seen.  And our bolding interface then makes
    this available to the user.
  </p>

  <p>
    In fact, the model captures many other heuristics.  For instance,
    in the above examples the heights of the fonts are (roughly)
    preserved, which is the norm in professional font design. Again,
    what’s going on isn’t just a thickening of the font, but rather
    the application of a more subtle heuristic inferred by the
    generative model.  Such heuristics can be used to create fonts
    with properties which would otherwise be unlikely to occur to
    users.  Thus, the tool expands ordinary people’s ability to
    explore the space of meaningful fonts.
  </p>

  <p>
    The font tool is an example of a kind of cognitive technology.  In
    particular, the primitive operations it contains can be
    internalized as part of how a user thinks.  In this it resembles a
    program such as <em>Photoshop</em> or a spreadsheet or 3D graphics
    programs.  Each provides a novel set of interface primitives,
    primitives which can be internalized by the user as fundamental
    new elements in their thinking. This act of internalization of new
    primitives is fundamental to much work on intelligence
    augmentation.
  </p>

  <p>
    The ideas shown in the font tool can be extended to other domains.
    Using the same interface, we can use a generative model to
    manipulate images of human faces using qualities such as
    expression, gender, or hair color. Or to manipulate sentences
    using length, sarcasm, or tone.  Or to manipulate molecules using
    chemical properties:
  </p>

  <style>
    #alternate-uses .root {
      max-width: 300px;
      margin: 0 auto;
      padding: 0 20px;
    }
    @media(min-width: 600px) {
      #alternate-uses .root {
        max-width: 760px;
        display: grid;
        grid-template-columns: 1fr 1fr 1fr;
        grid-gap: 50px;
      }
    }
    .note {
      margin-top: 12px;
      grid-column-end: span 2;
      font-size: 10px;
      line-height: 1.5em;
      text-align: left;
      color: rgba(0, 0, 0, 0.4);
    }
  </style>
  <figure id="alternate-uses" style="grid-column: screen">

    <div class="root">
      <div class="column">
        <div class="faces"></div>
        <div class="note">
          Images from <em>Sampling Generative Networks</em> by <d-cite key="White2016a">White</d-cite>.
        </div>
      </div>
      <div class="column">
        <div class="sentences"></div>
        <div class="note">
          Sentence from <em>Pride and Prejudice</em> by Jane Austen. Interpolated by the authors. Inspired by experiments done by the novelist <d-cite key="Sloan2017">Robin Sloan</d-cite>
        </div>
      </div>
      <div class="column">
        <div class="molecules"></div>
        <div class="note">
          Images from <em>Automatic chemical design using a data-driven continuous representation of molecules</em> by <d-cite key="GomezBombarelli2016">Gómez-Bombarelli <em>et al</em></d-cite>.
        </div>
      </div>


    </div>
  </figure>

  <p>
    Such generative interfaces provide a kind of cartography of
    generative models, ways for humans to explore and make meaning
    using those models.
  </p>

  <p>
    We saw earlier that the font model automatically infers relatively
    deep principles about font design, and makes them available to
    users.  While it’s great that such deep principles can be
    inferred, sometimes such models infer other things that are wrong,
    or undesirable.  For example, <d-cite key="White2016a">White
    points out</d-cite> the addition of a smile vector in some face
    models will make faces not just smile more, but also appear more
    feminine.  Why?  Because in the training data more women than men
    were smiling.  So these models may not just learn deep facts about
    the world, they may also internalize prejudices or erroneous
    beliefs.  Once such a bias is known, it is often possible to make
    corrections.  But to ﬁnd those biases requires careful auditing
    of the models, and it is not yet clear how we can ensure such
    audits are exhaustive.
  </p>

  <!-- <p>
    The font tool and similar interfaces are severely limited by the
    quality of the underlying models.  This of course suggests the
    question of how to build higher quality generative models.  A lot
    of ongoing research work is now focused on this, and the models
    are improving rapidly.  But as the models improve, we believe it
    is at least equally interesting to ask how to build interfaces
    that surface what those models have learned.
  </p> -->

  <p>
    More broadly, we can ask why attribute vectors work, when they
    work, and when they fail? At the moment, the answers to these
    questions are poorly understood.
  </p>

  <p>
    For the attribute vector to work requires that taking any starting
    font, we can construct the corresponding bold version by adding
    the <em>same</em> vector in the latent space.  However, <em>a
    priori</em> there is no reason using a single constant vector to
    displace will work. It may be that we should displace in many
    different ways.  For instance, the heuristics used to bold serif
    and sans-serif fonts are quite different, and so it seems likely
    that very different displacements would be involved:
  </p>

  <figure id="boldify-parallel" class="l-page tight">
  </figure>

  <p>
    Of course, we could do something more sophisticated than using a
    single constant attribute vector.  Given pairs of example fonts
    (unbold, bold) we could train a machine learning algorithm to take
    as input the latent vector for the unbolded version and output the
    latent vector for the bolded version.  With additional training
    data about font weights, the machine learning algorithm could
    learn to generate fonts of arbitrary weight.  Attribute vectors
    are just an extremely simple approach to doing these kinds of
    operation.
  </p>

  <p>
    For these reasons, it seems unlikely that attribute vectors will
    last as an approach to manipulating high-level features.  Over the
    next few years much better approaches will be developed.  However,
    we can still expect interfaces offering operations broadly similar
    to those sketched above, allowing access to high-level and
    potentially user-deﬁned concepts.  That interface pattern doesn’t
    depend on the technical details of attribute vectors.
  </p>


  <h2>
    Interactive Generative Adversarial Models
  </h2>

  <p>
    Let’s look at another example using machine learning models to
    augment human creativity. It’s the interactive generative
    adversarial networks, or iGANs, introduced
    by <d-cite key="Zhu2016a">Zhu <em>et al</em></d-cite> in 2016.
  </p>

  <p>
    One of the examples of Zhu <em>et al</em> is the use of iGANs in
    an interface to generate images of consumer products such as
    shoes.  Conventionally, such an interface would require the
    programmer to write a program containing a great deal of knowledge
    about shoes: soles, laces, heels, and so on.  Instead of doing
    this, Zhu <em>et al</em> train a generative model using <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">50</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathrm">5</span><span class="mord mathrm">0</span></span></span></span></span>
    thousand images of shoes, downloaded from Zappos. They then use
    that generative model to build an interface that lets a user
    roughly sketch the shape of a shoe, the sole, the laces, and so
    on:
  </p>

  <figure name="RedShoeGif">
    <video controls="" src="videos/shoe-sketch.qt" width="100%" poster="videos/shoe-sketch-poster.jpg"></video>
    <figcaption>Excerpted from <d-cite key="Zhu2016a">Zhu <em>et
    al</em></d-cite>.</figcaption>
  </figure>


  <p>
    The visual quality is low, in part because the generative model
    Zhu <em>et al</em> used is outdated by modern (2017) standards
    – with more modern models, the visual quality would be much
    higher.
  </p>

  <p>
    But the visual quality is not the point.  Many interesting things
    are going on in this prototype.  For instance, notice how the
    overall shape of the shoe changes considerably when the sole is
    ﬁlled in – it becomes narrower and sleeker.  Many small
    details are ﬁlled in, like the black piping on the top of the
    white sole, and the red coloring ﬁlled in everywhere on the
    shoe’s upper.  These and other facts are automatically deduced
    from the underlying generative model, in a way we’ll describe
    shortly.
  </p>

  <p>
    The same interface may be used to sketch landscapes. The only
    difference is that the underlying generative model has been
    trained on landscape images rather than images of shoes. In this
    case it becomes possible to sketch in just the colors associated
    to a landscape. For example, here’s a user sketching in some green
    grass, the outline of a mountain, some blue sky, and snow on the
    mountain:
  </p>

  <figure name="LandscapeSketch">
    <video controls="" src="videos/mountain-sketch.qt" width="100%" poster="videos/mountain-sketch-poster.jpg"></video>
    <figcaption>Excerpted from <d-cite key="Zhu2016a">Zhu <em>et
    al</em></d-cite>.</figcaption>
  </figure>

  <p>
    The generative models used in these interfaces are different than
    for our font model. Rather than using variational autoencoders,
    they’re based on <d-cite key="Goodfellow2014a">generative
    adversarial networks (GANs)</d-cite>.  But the underlying idea is
    still to ﬁnd a low-dimensional latent space which can be used to
    represent (say) all landscape images, and map that latent space to
    a corresponding image.  Again, we can think of points in the
    latent space as a compact way of describing landscape images.
  </p>

  <p>
    Roughly speaking, the way the iGANs works is as follows. Whatever
    the current image is, it corresponds to some point in the latent
    space:
  </p>

  <figure id="igans-basic" class="tight">
  </figure>

  <p>
    Suppose, as happened in the earlier video, the user now sketches
    in a stroke outlining the mountain shape.  We can think of the
    stroke as a constraint on the image, picking out a subspace of the
    latent space, consisting of all points in the latent space whose
    image matches that outline:
  </p>

  <figure id="igans-constraint" class="tight">
  </figure>

  <p>
    The way the interface works is to ﬁnd a point in the latent space
    which is near to the current image, so the image is not changed
    too much, but also coming close to satisfying the imposed
    constraints.  This is done by optimizing an objective function
    which combines the distance to each of the imposed constraints, as
    well as the distance moved from the current point.  If there’s
    just a single constraint, say, corresponding to the mountain
    stroke, this looks something like the following:
  </p>

  <figure id="igans-minimization" class="tight">
  </figure>

  <p>
    We can think of this, then, as a way of applying constraints to
    the latent space to move the image around in meaningful ways.
  </p>

  <p>
    The iGANs have much in common with the font tool we showed
    earlier.  Both make available operations that encode much subtle
    knowledge about the world, whether it be learning to understand
    what a mountain looks like, or inferring that enclosed negative
    space should be preserved when bolding a font.  Both the iGANs and
    the font tool provide ways of understanding and navigating a
    high-dimensional space, keeping us on the natural space of fonts
    or shoes or landscapes. As Zhu <em>et al</em> remark:
  </p>

  <blockquote>
    [F]or most of us, even a simple image manipulation in Photoshop
    presents insurmountable difﬁculties… any less-than-perfect
    edit immediately makes the image look completely unrealistic. To
    put another way, classic visual manipulation paradigm does not
    prevent the user from “falling off” the manifold of
    natural images.
  </blockquote>

  <p>
    Like the font tool, the iGANs is a cognitive technology.  Users
    can internalize the interface operations as new primitive elements
    in their thinking.  In the case of shoes, for example, they can
    learn to think in terms of the difference they want to apply,
    adding a heel, or a higher top, or a special highlight.  This is
    richer than the traditional way non-experts think about shoes
    (“Size 11, black” <em>etc</em>).  To the extent that
    non-experts do think in more sophisticated ways –
    “make the top a little higher and sleeker” –
    they get little practice in thinking this way, or seeing the
    consequences of their choices. Having an interface like this
    enables easier exploration, the ability to develop idioms and the
    ability to plan, to swap ideas with friends, and so on.
  </p>

  <h2>
    Two models of computation
  </h2>

  <p>
    Let’s revisit the question we began the essay with, the question
    of what computers are for, and how this relates to intelligence
    augmentation.
  </p>

  <p>
    One common conception of computers is that they’re problem-solving
    machines: “computer, what is the result of ﬁring this
    artillery shell in such-and-such a wind [and so on]?”;
    “computer, what will the maximum temperature in Tokyo be in
    5 days?”; “computer, what is the best move to take
    when the Go board is in this position?”; “computer,
    how should this image be classiﬁed?”; and so on.
  </p>

  <p>
    This is a conception common to both the early view of computers as
    number-crunchers, and also in much work on AI, both historically
    and today.  It’s a model of a computer as a way of outsourcing
    cognition.  In speculative depictions of possible future AI,
    this <em>cognitive outsourcing</em> model often shows up in the
    view of an AI as an oracle, able to solve some large class of
    problems with better-than-human performance.
  </p>

  <p>
    But a very different conception of what computers are for is
    possible, a conception much more congruent with work on
    intelligence augmentation.
  </p>

  <p>
    To understand this alternate view, consider our subjective
    experience of thought. For many people, that experience is verbal:
    they think using language, forming chains of words in their heads,
    similar to sentences in speech or written on a page.  For other
    people, thinking is a more visual experience, incorporating
    representations such as graphs and maps.  Still other people mix
    mathematics into their thinking, using algebraic expressions or
    diagrammatic techniques, such as Feynman diagrams and Penrose
    diagrams.
  </p>

  <p>
    In each case, we’re thinking using representations invented by
    other people: words, graphs, maps, algebra, mathematical diagrams,
    and so on.  We internalize these cognitive technologies as we grow
    up, and come to use them as a kind of substrate for our thinking.
  </p>

  <p>
    For most of history, the range of available cognitive technologies
    has changed slowly and incrementally. A new word will be
    introduced, or a new mathematical symbol. More rarely, a radical
    new cognitive technology will be developed.  For example, in 1637
    Descartes published his “Discourse on Method”,
    explaining how to represent geometric ideas using algebra, and
    vice versa:
   </p>

   <figure id="descartes"></figure>

  <p>
    This enabled a radical change and expansion in how we think about
    both geometry and algebra.
   </p>

  <p>
    Historically, lasting cognitive technologies have been invented
    only rarely.  But modern computers are a meta-medium enabling the
    rapid invention of many new cognitive technologies.  Consider a
    relatively banal example, such
    as <em>Photoshop</em>. Adept <em>Photoshop</em> users routinely
    have formerly impossible thoughts such as: “let’s apply the
    clone stamp to the such-and-such layer.”.  That’s an
    instance of a more general class of thought: “computer, [new
    type of action] this [new type of representation for a newly
    imagined class of object]”. When that happens, we’re using
    computers to expand the range of thoughts we can think.
  </p>

  <p>
    It’s this kind of <em>cognitive transformation</em> model which
    underlies much of the deepest work on intelligence augmentation.
    Rather than outsourcing cognition, it’s about changing the
    operations and representations we use to think; it’s about
    changing the substrate of thought itself.  And so while cognitive
    outsourcing is important, this cognitive transformation view
    offers a much more profound model of intelligence augmentation.
    It’s a view in which computers are a means to change and expand
    human thought itself.
  </p>

  <p>
    Historically, cognitive technologies were developed by human
    inventors, ranging from the invention of writing in Sumeria and
    Mesoamerica, to the modern interfaces of designers such as Douglas
    Engelbart, Alan Kay, and others.
  </p>

  <p>
    Examples such as those described in this essay suggest that AI
    systems can enable the creation of new cognitive technologies.
    Things like the font tool aren’t just oracles to be consulted when
    you want a new font.  Rather, they can be used to explore and
    discover, to provide new representations and operations, which can
    be internalized as part of the user’s own thinking. And while
    these examples are in their early stages, they suggest AI is not
    just about cognitive outsourcing.  A different view of AI is
    possible, one where it helps us invent new cognitive technologies
    which transform the way we think.
  </p>

    <p>
    In this essay we’ve focused on a small number of examples, mostly
    involving exploration of the latent space.  There are many other
    examples of artiﬁcial intelligence augmentation.  To give some
    ﬂavor, without being comprehensive:
    the <d-cite key="Ha2017">sketch-rnn system</d-cite>, for neural
    network assisted drawing;
    the <d-cite key="Fiebrink2011">Wekinator</d-cite>, which enables
    users to rapidly build new musical instruments and artistic
    systems; <d-cite key="Loh2017">TopoSketch</d-cite>, for developing
    animations by exploring latent spaces; machine learning models for
    designing <d-cite key="Gold2016">overall typographic
    layout</d-cite>; and a generative model which enables
    interpolation between <d-cite key="Roberts2017">musical
    phrases</d-cite>.  In each case, the systems use machine learning
    to enable new primitives which can be integrated into the user’s
    thinking.  More broadly, artiﬁcial intelligence augmentation will
    draw on ﬁelds such as <d-cite key="Colton2012">computational
    creativity</d-cite> and <d-cite key="Ware2001">interactive machine
    learning</d-cite>.
  </p>

  <h2>
    Finding powerful new primitives of thought
  </h2>

  <p>
    We’ve argued that machine learning systems can help create
    representations and operations which serve as new primitives in
    human thought.  What properties should we look for in such new
    primitives?  This is too large a question to be answered
    comprehensively in a short essay.  But we will explore it brieﬂy.
  </p>

  <p>
    Historically, important new media forms often seem strange when
    introduced.  Many such stories have passed into popular culture:
    the near riot at the premiere of Stravinsky and Nijinksy’s
    “Rite of Spring”; the consternation caused by the
    early cubist paintings, leading
    <em>The New York Times</em> <d-cite key="NYT1911">to
      comment</d-cite>: “What do they mean? Have those
    responsible for them taken leave of their senses? Is it art or
    madness? Who knows?”
  </p>

  <p>
    Another example comes from physics.  In the 1940s, different
    formulations of the theory of quantum electrodynamics were
    developed independently by the physicists Julian Schwinger,
    Shin’ichirō Tomonaga, and Richard Feynman.  In their work,
    Schwinger and Tomonaga used a conventional algebraic approach,
    along lines similar to the rest of physics. Feynman used a more
    radical approach, based on what are now known as Feynman diagrams,
    for depicting the interaction of light and matter:
  </p>

  <figure class="grid" style="grid-template-columns: 3fr 1fr;">
    <img src="images/feynmann-diagram.svg" style="max-width: 400px">
    <figcaption>Image by <a href="https://commons.wikimedia.org/w/index.php?curid=1764161">Joel
	Holdsworth</a>), licensed under a Creative Commons
	Attribution-Share Alike 3.0 Unported license
    </figcaption>
  </figure>

  <p>
    Initially, the Schwinger-Tomonaga approach was easier for other
    physicists to understand.  When Feynman and Schwinger presented
    their work at a 1948 workshop, Schwinger was immediately
    acclaimed. By contrast, Feynman left his audience mystiﬁed.  As
    James Gleick put it in his <d-cite key="Gleick1992a">biography of
      Feynman</d-cite>:
  </p>

  <blockquote>
    It struck Feynman that everyone had a favorite principle or
    theorem and he was violating them all… Feynman knew he had
    failed. At the time, he was in anguish.  Later he said simply:
    “I had too much stuff. My machines came from too far
    away.”
  </blockquote>

  <p>
    Of course, strangeness for strangeness’s sake alone is not
    useful. But these examples suggest that breakthroughs in
    representation often appear strange at ﬁrst. Is there any
    underlying reason that is true?
  </p>

  <p>
    Part of the reason is because if some representation is truly new,
    then it will appear different than anything you’ve ever seen
    before.  Feynman’s diagrams, Picasso’s paintings, Stravinsky’s
    music: all revealed genuinely new ways of making meaning.  Good
    representations sharpen up such insights, eliding the familiar to
    show that which is new as vividly as possible.  But because of
    that emphasis on unfamiliarity, the representation will seem
    strange: it shows relationships you’ve never seen before.  In some
    sense, the task of the designer is to identify that core
    strangeness, and to amplify it as much as possible.
  </p>

  <p>
    Strange representations are often difﬁcult to understand.  At
    ﬁrst, physicists preferred Schwinger-Tomonaga to Feynman.  But as
    Feynman’s approach was slowly understood by physicists, they
    realized that although Schwinger-Tomonaga and Feynman were
    mathematically equivalent, Feynman was more powerful.  As Gleick
    puts it:
  </p>

  <blockquote>
    Schwinger’s students at Harvard were put at a competitive
    disadvantage, or so it seemed to their fellows elsewhere, who
    suspected them of surreptitiously using the diagrams anyway.  This
    was sometimes true… Murray Gell-Mann later spent a semester
    staying in Schwinger’s house and loved to say afterward that he
    had searched everywhere for the Feynman diagrams.  He had not
    found any, but one room had been locked…
  </blockquote>

  <p>
    These ideas are true not just of historical representations, but
    also of computer interfaces.  However, our advocacy of strangeness
    in representation contradicts much conventional wisdom about
    interfaces, especially the widely-held belief that they should be
    “user friendly”, i.e., simple and immediately useable
    by novices. That most often means the interface is cliched, built
    from conventional elements combined in standard ways. But while
    using a cliched interface may be easy and fun, it’s an ease
    similar to reading a formulaic romance novel. It means the
    interface does not reveal anything truly surprising about its
    subject area. And so it will do little to deepen the user’s
    understanding, or to change the way they think.  For mundane tasks
    that is ﬁne, but for deeper tasks, and for the longer term, you
    want a better interface.
  </p>

  <p>
    Ideally, an interface will surface the deepest principles
    underlying a subject, revealing a new world to the user.  When you
    learn such an interface, you internalize those principles, giving
    you more powerful ways of reasoning about that world.  Those
    principles are the diffs in your understanding.  They’re all you
    really want to see, everything else is at best support, at worst
    unimportant dross.  The purpose of the best interfaces isn’t to be
    user-friendly in some shallow sense.  It’s to be user-friendly in
    a much stronger sense, <d-cite key="Nielsen2016b">reifying deep
    principles</d-cite> about the world, making them the working
    conditions in which users live and create. At that point what once
    appeared strange can instead becomes comfortable and familiar,
    part of the pattern of thought<d-footnote>A powerful instance of
    these ideas is when an interface reiﬁes general-purpose
    principles. An example is an
    interface <d-cite key="Nielsen2016b">one of us</d-cite> developed
    based on the principle of conservation of energy.  Such
    general-purpose principles generate multiple unexpected
    relationships between the entities of a subject, and so are a
    particularly rich source of insights when reiﬁed in an
    interface.</d-footnote>.
  </p>

  <p>
    What does this mean for the use of AI models for intelligence
    augmentation?
  </p>

  <p>
    Aspirationally, as we’ve seen, our machine learning models will
    help us build interfaces which reify deep principles in ways
    meaningful to the user.  For that to happen, the models have to
    discover deep principles about the world, recognize those
    principles, and then surface them as vividly as possible in an
    interface, in a way comprehensible by the user.
  </p>

  <p>
    Of course, this is a tall order! The examples we’ve shown are just
    barely beginning to do this. It’s true that our models do
    sometimes discover relatively deep principles, like the
    preservation of enclosed negative space when bolding a font. But
    this is merely implicit in the model. And while we’ve built a tool
    which takes advantage of such principles, it’d be better if the
    model automatically inferred the important principles learned, and
    found ways of explicitly surfacing them through the interface.
    (Encouraging progress toward this has been made
    by <d-cite key="Chen2016">InfoGANs</d-cite>, which use
    information-theoretic ideas to ﬁnd structure in the latent
    space.)  Ideally, such models would start to get at true
    explanations, not just in a static form, but in a dynamic form,
    manipulable by the user.  But we’re a long way from that point.
  </p>

  <h2>
    Do these interfaces inhibit creativity?
  </h2>

  <p>
    It’s tempting to be skeptical of the expressiveness of the
    interfaces we’ve described.  If an interface constrains us to
    explore only the natural space of images, does that mean we’re
    merely doing the expected?  Does it mean these interfaces can only
    be used to generate visual cliches?  Does it prevent us from
    generating anything truly new, from doing truly creative work?
  </p>

  <p>
    To answer these questions, it’s helpful to identify two different
    modes of creativity.  This two-mode model is over-simpliﬁed:
    creativity doesn’t ﬁt so neatly into two distinct categories. Yet
    the model nonetheless clariﬁes the role of new interfaces in
    creative work.
  </p>

  <p>
    The ﬁrst mode of creativity is the everyday creativity of a
    craftsperson engaged in their craft.  Much of the work of a font
    designer, for example, consists of competent recombination of the
    best existing practices. Such work typically involves many
    creative choices to meet the intended design goals, but not
    developing key new underlying principles.
  </p>

  <p>
    For such work, the generative interfaces we’ve been discussing are
    promising. While they currently have many limitations, future
    research will identity and ﬁx many deﬁciencies. This is
    happening rapidly with GANs: the original
    GANs<d-cite key="Goodfellow2014a"></d-cite> had many limitations,
    but models soon appeared that were better adapted to
    images<d-cite key="Radford2016a"></d-cite>, improved the
    resolution, reduced artifacts<d-footnote>So much work has been
    done on improving resolution and reducing artifacts it seems
    unfair to single out any small set of papers, and to omit the many
    others.</d-footnote>, and so on.  With enough iterations it’s
    plausible these generative interfaces will become powerful tools
    for craft work.
  </p>

  <p>
    The second mode of creativity aims toward developing new
    principles that fundamentally change the range of creative
    expression. One sees this in the work of artists such as Picasso
    or Monet, who violated existing principles of painting, developing
    new principles which enabled people to see in new ways.
  </p>

  <p>
    Is it possible to do such creative work, while using a generative
    interface?  Don’t such interfaces constrain us to the space of
    natural images, or natural fonts, and thus actively prevent us
    from exploring the most interesting new directions in creative
    work?
  </p>

  <p>
    The situation is more complex than this.
  </p>

  <p>
    In part, this is a question about the power of our generative
    models. In some cases, the model can only generate recombinations
    of existing ideas. This is a limitation of an ideal GAN, since a
    perfectly trained GAN generator will reproduce the training
    distribution.  Such a model can’t directly generate an image based
    on new fundamental principles, because such an image wouldn’t look
    anything like it’s seen in its training data.
  </p>

  <p>
    Artists such as <a href="http://quasimondo.com/">Mario
      Klingemann</a> and <a href="http://www.miketyka.com/">Mike
      Tyka</a> are now using GANs to create interesting
      artwork. They’re doing that using “imperfect” GAN
      models, which they seem to be able to use to explore interesting
      new principles; it’s perhaps the case that bad GANs may be more
      artistically interesting than ideal GANs.  Furthermore, nothing
      says an interface must only help us explore the latent space.
      Perhaps operations can be added which deliberately take us out
      of the latent space, or to less probable (and so more
      surprising) parts of the space of natural images.
  </p>

  <p>
    Of course, GANs are not the only generative models. In a
    sufﬁciently powerful generative model, the generalizations
    discovered by the model may contain ideas going beyond what humans
    have discovered. In that case, exploration of the latent space may
    enable us to discover new fundamental principles.  The model would
    have discovered stronger abstractions than human experts.  Imagine
    a generative model trained on paintings up until just before the
    time of the cubists; might it be that by exploring that model it
    would be possible to discover cubism? It would be an analogue to
    something like the prediction of Bose-Einstein condensation, as
    discussed earlier in the essay. Such invention is beyond today’s
    generative models, but seems a worthwhile aspiration for future
    models.
  </p>

  <p>
    Our examples so far have all been based on generative models.  But
    there are some illuminating examples which are not based on
    generative models.  Consider the pix2pix system developed
    by <d-cite key="Isola2017a">Isola <em>et al</em></d-cite>. This
    system is trained on pairs of images, e.g., pairs showing the
    edges of a cat, and the actual corresponding cat.  Once trained,
    it can be shown a set of edges and asked to generate an image for
    an actual corresponding cat.  It often does this quite well:
  </p>

  <style>
    .cat-grid .row {
      grid-template-columns: 1fr 1fr 0.5fr;
      align-items: center;
    }
    .cat-grid .row {
      border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    }
    .cat-grid .row:last-child {
      border-bottom: none;
    }
  </style>

  <figure class="cat-grid">
    <div class="row grid">
      <h4>Input</h4>
      <h4>Output</h4>
      <div></div>
    </div>
    <div class="row grid">
      <img src="images/cat-sample-input.jpg" alt="">
      <img src="images/cat-sample-output.jpg" alt="">
      <figcaption>
        <a href="https://affinelayer.com/pixsrv/">Live demo by Christopher Hesse</a>
      </figcaption>
    </div>
  </figure>

  <p>
    When supplied with unusual constraints, pix2pix can produce
    striking images:
  </p>

  <figure class="cat-grid">
    <div class="row grid">
      <img src="images/bread-cat-input.jpg" alt="">
      <img src="images/bread-cat-output.jpg" alt="">
      <figcaption><a href="https://twitter.com/ivymyt/status/834174687282241537">Bread cat by Ivy Tsai</a></figcaption>
    </div>
    <div class="row grid">
      <img src="images/cat-beholder-input.jpg" alt="">
      <img src="images/cat-beholder-output.jpg" alt="">
      <figcaption><a href="https://affinelayer.com/pixsrv/beholder.jpg">Cat beholder by Marc Hesse</a></figcaption>
    </div>
    <div class="row grid">
      <img src="images/spiral-cat-input.jpg" alt="">
      <img src="images/spiral-cat-output.jpg" alt="">
      <figcaption>Spiral cat</figcaption>
    </div>
  </figure>

  <p>
    This is perhaps not high creativity of a Picasso-esque level.  But
    it is still surprising.  It’s certainly unlike images most of us
    have ever seen before. How does pix2pix and its human user achieve
    this kind of result?
  </p>

  <p>
    Unlike our earlier examples, pix2pix is not a generative model.
    This means it does not have a latent space or a corresponding
    space of natural images.  Instead, there is a neural network,
    called, confusingly, a generator – this is not meant in the
    same sense as our earlier generative models – that takes as
    input the constraint image, and produces as output the ﬁlled-in
    image.
  </p>

  <p>
    The generator is trained adversarially against a discriminator
    network, whose job is to distinguish between pairs of images
    generated from real data, and pairs of images generated by the
    generator.
  </p>

  <p>
    While this sounds similar to a conventional GAN, there is a
    crucial difference: there is no latent vector input to the
    generator<d-footnote>Actually, <dt-cite key="Isola2017a">Isola <em>et
    al</em></dt-cite> experimented with adding such a latent vector to
    the generator, but found it made little difference to the
    resulting images.</d-footnote>. Rather, there is simply an input
    constraint. When a human inputs a constraint unlike anything seen
    in training, the network is forced to improvise, doing the best it
    can to interpret that constraint according to the rules it has
    previously learned. The creativity is the result of a forced
    merger of knowledge inferred from the training data, together with
    novel constraints provided by the user.  As a result, even
    relatively simple ideas – like the bread- and beholder-cats
    – can result in striking new types of images, images not
    within what we would previously have considered the space of
    natural images.
  </p>

  <h2>Conclusion</h2>


  <p>
    It is conventional wisdom that AI will change how we interact with
    computers.  Unfortunately, many in the AI community greatly
    underestimate the depth of interface design, often regarding it as
    a simple problem, mostly about making things pretty or
    easy-to-use. In this view, interface design is a problem to be
    handed off to others, while the hard work is to train some machine
    learning system.
  </p>

  <p>
    This view is incorrect. At its deepest, interface design means
    developing the fundamental primitives human beings think and
    create with.  This is a problem whose intellectual genesis goes
    back to the inventors of the alphabet, of cartography, and of
    musical notation, as well as modern giants such as Descartes,
    Playfair, Feynman, Engelbart, and Kay. It is one of the hardest,
    most important and most fundamental problems humanity grapples
    with.
  </p>

  <p>
    As discussed earlier, in one common view of AI our computers will
    continue to get better at solving problems, but human beings will
    remain largely unchanged.  In a second common view, human beings
    will be modiﬁed at the hardware level, perhaps directly through
    neural interfaces, or indirectly through whole brain emulation.
  </p>

  <p>
    We’ve described a third view, in which AIs actually change
    humanity, helping us invent new cognitive technologies, which
    expand the range of human thought.  Perhaps one day those
    cognitive technologies will, in turn, speed up the development of
    AI, in a virtuous feedback cycle:
  </p>

   <figure>
     <img src="images/cycle.svg">
   </figure>

  <p>
    It would not be a Singularity in machines.  Rather, it would be a
    Singularity in humanity’s range of thought.  Of course, this loop
    is at present extremely speculative.  The systems we’ve described
    can help develop more powerful ways of thinking, but there’s at
    most an indirect sense in which those ways of thinking are being
    used in turn to develop new AI systems.
  </p>

  <p>
    Of course, over the long run it’s possible that machines will
    exceed humans on all or most cognitive tasks.  Even if that’s the
    case, cognitive transformation will still be a valuable end, worth
    pursuing in its own right.  There is pleasure and value involved
    in learning to play chess or Go well, even if machines do it
    better.  And in activities such as story-telling the beneﬁt often
    isn’t so much the artifact produced as the process of construction
    itself, and the relationships forged.  There is intrinsic value in
    personal change and growth, apart from instrumental beneﬁts.
  </p>

   <p>
     The interface-oriented work we’ve discussed is outside the
     narrative used to judge most existing work in artiﬁcial
     intelligence.  It doesn’t involve beating some benchmark for a
     classiﬁcation or regression problem.  It doesn’t involve
     impressive feats like beating human champions at games such as
     Go.  Rather, it involves a much more subjective and
     difﬁcult-to-measure criterion: is it helping humans think and
     create in new ways?
   </p>

  <p>
    This creates difﬁculties for doing this kind of work,
    particularly in a research setting.  Where should one publish?
    What community does one belong to?  What standards should be
    applied to judge such work?  What distinguishes good work from
    bad?
  </p>

  <p>
    We believe that over the next few years a community will emerge
    which answers these questions.  It will run workshops and
    conferences. It will publish work in venues such as Distill.  Its
    standards will draw from many different communities: from the
    artistic and design and musical communities; from the mathematical
    community’s taste in abstraction and good deﬁnition; as well as
    from the existing AI and IA communities, including work on
    computational creativity and human-computer interaction.  The
    long-term test of success will be the development of tools which
    are widely used by creators. Are artists using these tools to
    develop remarkable new styles?  Are scientists in other ﬁelds
    using them to develop understanding in ways not otherwise
    possible?  These are great aspirations, and require an approach
    that builds on conventional AI work, but also incorporates very
    different norms.
  </p>

</d-article>


<!-- <script src="http://localhost:8888/dist/template.v2.js"></script> -->
<script src="index.bundle.js"></script>
<d-appendix>

  <h3>Acknowledgments</h3>
  <p>
    We are extremely grateful to James Wexler who built and trained
    the model on which the font demos are based. Nikhil Thorat and
    Daniel Smilkov also generously optimized the browser
    implementation of the model, which runs
    using <a href="https://deeplearnjs.org/">deeplearn.js</a>.  Thanks
    to Daniel Dewey, Andy Matuschak, Robert Ochshorn, and Bret Victor
    for valuable conversations and feedback.
  </p>

  <h3>Author contribution</h3>

  <p>
    Authors are listed alphabetically in the byline.  Michael drafted
    the text of the essay, but the ideas of the essay were developed
    jointly in conversation with Shan.  Shan developed the interactive
    diagrams, with considerable input from Michael.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list distill-prerendered="true"><style>
d-citation-list {
  contain: layout style;
}

d-citation-list .references {
  grid-column: text;
}

d-citation-list .references .title {
  font-weight: 500;
}
</style><h3 id="references">References</h3><ol id="references-list" class="references"><li id="Engelbart1962a"><span class="title">Augmenting Human Intellect: A Conceptual Framework</span> <br>Engelbart, D.C., 1962. </li><li id="Wexler2017"><span class="title">deeplearn.js font demo</span>   <a href="https://deeplearnjs.org/demos/latent-space-explorer/">[link]</a><br>Wexler, J., 2017. </li><li id="Kingma2014a"><span class="title">Auto-encoding variational Bayes</span> <br>Kingma, D.P. and Welling, M., 2014. ICLR. </li><li id="Bernhardsson2016a"><span class="title">Analyzing 50k fonts using deep neural networks</span>   <a href="https://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deep-neural-networks.html">[HTML]</a><br>Bernhardsson, E., 2016. </li><li id="Larsen2016a"><span class="title">Autoencoding beyond pixels using a learned similarity metric</span> <br>Larsen, A.B.L., Sønderby, S.K., Larochelle, H. and Winther, O., 2016. ICML. </li><li id="White2016a"><span class="title">Sampling Generative Networks</span>   <a href="http://arxiv.org/pdf/1609.04468.pdf">[PDF]</a><br>White, T., 2016. </li><li id="Sloan2017"><span class="title">Writing with the Machine</span>   <a href="https://vimeo.com/232545219">[link]</a><br>Sloan, R., 2017. Eyeo. </li><li id="GomezBombarelli2016"><span class="title">Automatic chemical design using a data-driven continuous representation of molecules</span>   <a href="http://arxiv.org/pdf/1610.02415.pdf">[PDF]</a><br>Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato, J.M., Aguilera-Iparraguirre, J., Hirzel, T.D., Adams, R.P. and Aspuru-Guzik, A., 2016. </li><li id="Zhu2016a"><span class="title">Generative visual manipulation on the natural image manifold</span> <br>Zhu, J., Krähenbühl, P., Schechtman, E. and Efros, A.A., 2016. European Conference on Computer Vision (ECCV). </li><li id="Goodfellow2014a"><span class="title">Generative adversarial nets</span> <br>Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014. Advances in Neural Information Processing Systems (NIPS), pp. 2672-2680. </li><li id="Ha2017"><span class="title">A Neural Representation of Sketch Drawings</span>   <a href="http://arxiv.org/pdf/1704.03477.pdf">[PDF]</a><br>Ha, D. and Eck, D., 2017. </li><li id="Fiebrink2011"><span class="title">Real-time human interaction with supervised learning algorithms for music composition and performance</span> <br>Fiebrink, R., 2011. Princeton University PhD Thesis.</li><li id="Loh2017"><span class="title">TopoSketch: Drawing in Latent Space</span> <br>Loh, I. and White, T., 2017. NIPS Workshop on Machine Learning for Creativity and Design. </li><li id="Gold2016"><span class="title">Taking The Robots To Design School, Part 1</span>   <a href="http://www.jon.gold/2016/05/robot-design-school/">[link]</a><br>Gold, J., 2016. </li><li id="Roberts2017"><span class="title">Hierarchical Variational Autoencoders for Music</span>   <a href="https://nips2017creativity.github.io/doc/Hierarchical_Variational_Autoencoders_for_Music.pdf">[PDF]</a><br>Roberts, A., Engel, J. and Eck, D., 2017. NIPS Workshop on Machine Learning for Creativity and Design. </li><li id="Colton2012"><span class="title">Computational creativity: the final frontier?</span> <br>Colton, S. and Wiggins, G.A., 2012. ECAI. </li><li id="Ware2001"><span class="title">Interactive machine learning: letting users build classifiers</span> <br>Ware, M., Frank, E., Holmes, G., Hall, M. and Witten, I.H., 2001. International Journal of Human-Computer Studies, Vol 55, pp. 281-292. </li><li id="NYT1911"><span class="title">Eccentric School of Painting Increased Its Vogue in the Current Art Exhibition — What Its Followers Attempt to Do</span>   <a href="http://query.nytimes.com/mem/archive-free/pdf?res=9D02E2D71131E233A2575BC0A9669D946096D6CF&amp;mcubz=3">[link]</a><br>1911. The New York Times. </li><li id="Gleick1992a"><span class="title">Genius: The Life and Science of Richard Feynman</span> <br>Gleick, J., 1992. Vintage Books.</li><li id="Nielsen2016b"><span class="title">Thought as a Technology</span>   <a href="http://cognitivemedium.com/tat/index.html">[HTML]</a><br>Nielsen, M., 2016. </li><li id="Chen2016"><span class="title">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</span> <br>Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I. and Abbeel, P., 2016. NIPS. </li><li id="Radford2016a"><span class="title">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</span>   <a href="http://arxiv.org/pdf/1511.06434.pdf">[PDF]</a><br>Radford, A., Metz, L. and Chintala, S., 2016. ICLR. </li><li id="Isola2017a"><span class="title">Image-to-Image Translation with Conditional Adversarial Networks</span>   <a href="http://arxiv.org/pdf/1611.07004.pdf">[PDF]</a><br>Isola, P., Zhu, J., Zhou, T. and Efros, A.A., 2017. </li></ol></d-citation-list><distill-appendix>
<style>
  distill-appendix {
    contain: layout style;
  }

  distill-appendix .citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  distill-appendix > * {
    grid-column: text;
  }
</style>

    <h3 id="updates-and-corrections">Updates and Corrections</h3>
    <p>
    If you see mistakes or want to suggest changes, please <a href="https://github.com/distillpub/post--aia/issues/new">create an issue on GitHub</a>. </p>
    
    <h3 id="reuse">Reuse</h3>
    <p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a class="github" href="https://github.com/distillpub/post--aia">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”.</p>
    
    <h3 id="citation">Citation</h3>
    <p>For attribution in academic contexts, please cite this work as</p>
    <pre class="citation short">Carter &amp; Nielsen, "Using Artificial Intelligence to Augment Human Intelligence", Distill, 2017.</pre>
    <p>BibTeX citation</p>
    <pre class="citation long">@article{carter2017using,
  author = {Carter, Shan and Nielsen, Michael},
  title = {Using Artificial Intelligence to Augment Human Intelligence},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/aia},
  doi = {10.23915/distill.00009}
}</pre>
    </distill-appendix></d-appendix>
  <d-bibliography>
    <script type="text/bibtex">
      @article{Bernhardsson2016a,
        title={Analyzing 50k fonts using deep neural networks},
        author={Erik Bernhardsson},
        url={https://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deep-neural-networks.html},
        year={2016}
      }

      @article{Chen2016,
      title={InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
      author={Xi Chen and Yan Duan and Rein Houthooft and John Schulman and Ilya Sutskever and Pieter Abbeel},
      year={2016},
      journal={NIPS}
      }

      @article{Colton2012,
      title={Computational creativity: the final frontier?},
      author={Simon Colton and Geraint A. Wiggins},
      journal={ECAI},
      year={2012}
      }

      @report{Engelbart1962a,
        title={Augmenting Human Intellect: A Conceptual Framework},
        author={Douglas C. Engelbart},
        year={1962},
      }

      @book{Fiebrink2011,
      title={Real-time human interaction with supervised learning algorithms for music composition and performance},
      author={Rebecca Fiebrink},
      year={2011},
      publisher={Princeton University PhD Thesis}
      }

      @book{Gleick1992a,
        title={Genius: The Life and Science of Richard Feynman},
        author={James Gleick},
        publisher={Vintage Books},
        place={New York},
        year={1992}
      }

      @article{Gold2016,
      title={Taking The Robots To Design School, Part 1},
      author={Jon Gold},
      year={2016},
      url={http://www.jon.gold/2016/05/robot-design-school/}
      }

      @article{GomezBombarelli2016,
      title={Automatic chemical design using a data-driven continuous representation of molecules},
      author={Rafael Gómez-Bombarelli and David Duvenaud and José
      Miguel Hernández-Lobato and Jorge Aguilera-Iparraguirre and
      Timothy D. Hirzel and Ryan P. Adams and Alán Aspuru-Guzik},
      year={2016},
      url={https://arxiv.org/abs/1610.02415}
      }

      @article{Goodfellow2014a,
        title={Generative adversarial nets},
        author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
        year={2014},
        journal={Advances in Neural Information Processing Systems (NIPS)},
        pages={2672-2680}
      }

      @article{Ha2017,
      title={A Neural Representation of Sketch Drawings},
      author={David Ha and Douglas Eck},
      year={2017},
      url={https://arxiv.org/abs/1704.03477}
      }

      @article{Holzinger2016,
      title={Interactive machine learning for health informatics: when do we need the human-in-the-loop?},
      author={Andreas Holzinger},
      journal={Brain Informatics},
      year={2016},
      pages={119-131}
      }

      @article{Isola2017a,
      title={Image-to-Image Translation with Conditional Adversarial Networks},
      author={Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
      year={2017},
      url={https://arxiv.org/abs/1611.07004},
      conference={CVPR}
      }

      @article{Kingma2014a,
        title={Auto-encoding variational Bayes},
        author={Diederik P. Kingma and Max Welling},
        year={2014},
        journal={ICLR}
      }

      @article{Larsen2016a,
        title={Autoencoding beyond pixels using a learned similarity metric},
      author={Anders Boesen Lindbo Larsen and Søren Kaae Sønderby and
      Hugo Larochelle and Ole Winther},
        year={2016},
        journal={ICML}
      }

      @article{Loh2017,
      title={TopoSketch: Drawing in Latent Space},
      author={Ian Loh and Tom White},
      year={2017},
      journal={NIPS Workshop on Machine Learning for Creativity and Design}
      }

      @article{Nielsen2016a,
        title={Toward an exploratory medium for mathematics},
        author={Michael Nielsen},
        year={2016},
        url={http://cognitivemedium.com/emm/emm.html}
      }

      @article{Nielsen2016b,
      title={Thought as a Technology},
      author={Michael Nielsen},
      year={2016},
      url={http://cognitivemedium.com/tat/index.html}
      }

      @misc{NYT1911,
        title={Eccentric School of Painting Increased Its Vogue in the Current Art Exhibition &mdash;
        What Its Followers Attempt to Do},
        journal={The New York Times},
        url={http://query.nytimes.com/mem/archive-free/pdf?res=9D02E2D71131E233A2575BC0A9669D946096D6CF&mcubz=3},
        year={1911}
      }

      @article{Radford2016a,
        title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
        author={Alec Radford and Luke Metz and Soumith Chintala},
        journal={ICLR},
        url={https://arxiv.org/abs/1511.06434},
        year={2016}
      }

      @article{Roberts2017,
      title={Hierarchical Variational Autoencoders for Music},
      author={Adam Roberts and Jesse Engel and Douglas Eck},
      year={2017},
      journal={NIPS Workshop on Machine Learning for Creativity and Design},
      url={https://nips2017creativity.github.io/doc/Hierarchical_Variational_Autoencoders_for_Music.pdf}
      }

      @article{Sloan2017,
      title={Writing with the Machine},
      author={Robin Sloan},
      journal={Eyeo},
      year={2017},
      url={https://vimeo.com/232545219}
      }

      @article{Ware2001,
      title={Interactive machine learning: letting users build classifiers},
      author={Malcolm Ware and Eibe Frank and Geoffrey Holmes and Mark Hall
      and Ian H. Witten},
      journal={International Journal of Human-Computer Studies},
      volume={55},
      pages={281-292},
      year={2001}
      }

      @article{Wexler2017,
      title={deeplearn.js font demo},
      url={https://deeplearnjs.org/demos/latent-space-explorer/},
      author={James Wexler},
      year={2017}
      }

      @article{White2016a,
      title={Sampling Generative Networks},
      author={Tom White},
      url={https://arxiv.org/abs/1609.04468},
      year={2016}
      }

      @article{Zhu2016a,
        title={Generative visual manipulation on the natural image manifold},
        author={Jun-Yan Zhu and Philipp Krähenbühl and Eli Schechtman
        and Alexei A. Efros},
        journal={European Conference on Computer Vision (ECCV)},
        year={2016},
      }


    </script>
  </d-bibliography>

<distill-footer></distill-footer><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-83741880-1', 'auto');
  ga('send', 'pageview');
</script></body>
<!-- Mirrored from distill.pub/2017/aia/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 04 Feb 2018 15:37:20 GMT -->
</html>